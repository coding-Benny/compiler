{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0094e25a",
   "metadata": {},
   "source": [
    "- movie review database를 이용한 sentiment analysis\n",
    "- vocabulary = feature: positive, negative\n",
    "- document = words list + 'pos'/'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163d9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ca13d",
   "metadata": {},
   "source": [
    "### frequency distribution(빈도수 분포) 확인 -> 가장 자주 나타나는 단어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54654013",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce47241a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583820"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6c59e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 77717),\n",
       " ('the', 76529),\n",
       " ('.', 65876),\n",
       " ('a', 38106),\n",
       " ('and', 35576),\n",
       " ('of', 34123),\n",
       " ('to', 31937),\n",
       " (\"'\", 30585),\n",
       " ('is', 25195),\n",
       " ('in', 21822),\n",
       " ('s', 18513),\n",
       " ('\"', 17612),\n",
       " ('it', 16107),\n",
       " ('that', 15924),\n",
       " ('-', 15595),\n",
       " (')', 11781),\n",
       " ('(', 11664),\n",
       " ('as', 11378),\n",
       " ('with', 10792),\n",
       " ('for', 9961)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fwds = FreqDist(all_words)\n",
    "fwds.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "puncs = [' ', ',', '.', ';', \"'\", '--', '-', ':', '(', ')', '[', ']']\n",
    "for x in puncs:\n",
    "    stop_words.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175081d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583820\n",
      "735506\n"
     ]
    }
   ],
   "source": [
    "filteredWords = []\n",
    "for w in all_words:\n",
    "    if w not in stop_words:\n",
    "        filteredWords.append(w)\n",
    "print(len(all_words))\n",
    "print(len(filteredWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d49e0d",
   "metadata": {},
   "source": [
    "### 자주 나타나는 3000개의 단어로 word feature 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e629af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwds = FreqDist(filteredWords)\n",
    "word_features = list(fwds.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da8c19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de36f9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'church',\n",
       " 'party',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'get']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a083e",
   "metadata": {},
   "source": [
    "### positive/negative documents에서 3000개의 단어에 속하는 feature 단어 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "053e4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31698c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = find_features(movie_reviews.words('neg/cv000_29416.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145acce8",
   "metadata": {},
   "source": [
    "### feature set 찾기: pos, neg에 대한 모든 review 문서에 대해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842104ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f885a6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51635459",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier를 이용한 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468de8e",
   "metadata": {},
   "source": [
    "- data = 2000\n",
    "- training = 1900\n",
    "- test = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587dffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = feature_sets[:1900]\n",
    "test_data = feature_sets[1900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9b93a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3d3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "clf = nltk.NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b747243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent:  85.0\n"
     ]
    }
   ],
   "source": [
    "print('Classifier accuracy percent: ', (nltk.classify.accuracy(clf, test_data)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f0e18a",
   "metadata": {},
   "source": [
    "#### 가장 가치있는 feature 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f34decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   sucks = True              neg : pos    =     10.5 : 1.0\n",
      "                 frances = True              pos : neg    =      9.1 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.2 : 1.0\n",
      "                  regard = True              pos : neg    =      7.0 : 1.0\n",
      "              schumacher = True              neg : pos    =      7.0 : 1.0\n",
      "                  crappy = True              neg : pos    =      7.0 : 1.0\n",
      "                    mena = True              neg : pos    =      7.0 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.0 : 1.0\n",
      "                  suvari = True              neg : pos    =      7.0 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.0 : 1.0\n",
      "                  turkey = True              neg : pos    =      6.5 : 1.0\n",
      "                 singers = True              pos : neg    =      6.4 : 1.0\n",
      "                   jumbo = True              neg : pos    =      6.3 : 1.0\n",
      "                 kidding = True              neg : pos    =      6.3 : 1.0\n",
      "               atrocious = True              neg : pos    =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f649b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_clf = open('naivebayes.pickle', 'wb')\n",
    "pickle.dump(clf, save_clf)\n",
    "save_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01b86e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_clf = open('naivebayes.pickle', 'rb')\n",
    "clf = pickle.load(re_clf)\n",
    "re_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e7ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benny",
   "language": "python",
   "name": "benny"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
