{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbe407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e82a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485ebdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_list(aList):\n",
    "    i = 1\n",
    "    for element in aList:\n",
    "        print(i, ')', element)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528743a2",
   "metadata": {},
   "source": [
    "## 1. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "171eb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d678d71",
   "metadata": {},
   "source": [
    "### 1) '.'로 문장 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c968d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = 'Today is a great day. It is even better than yesterday. And yesterday was the best day ever.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9cd895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- sentence list ----\n",
      "1 ) Today is a great day.\n",
      "2 ) It is even better than yesterday.\n",
      "3 ) And yesterday was the best day ever.\n"
     ]
    }
   ],
   "source": [
    "print('---- sentence list ----')\n",
    "sent_list = sent_tokenize(EXAMPLE_TEXT)\n",
    "show_list(sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3bbbb",
   "metadata": {},
   "source": [
    "### 2) '?', '!' 등으로 문장 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6f1fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Hi, how are you?\n",
      "2 ) I'm good, you?\n",
      "3 ) Great!\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Hi, how are you? I'm good, you? Great!\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf31d7",
   "metadata": {},
   "source": [
    "### 3) 약어(Mr., Mrs., ..) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29dcbfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Last night, I went to Mrs. Martinez's housewarming.\n",
      "2 ) It was a disaster.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Last night, I went to Mrs. Martinez's housewarming. It was a disaster.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95171f3b",
   "metadata": {},
   "source": [
    "### 4) 한글 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8110d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 안녕하세요.\n",
      "2 ) 좋은 아침입니다.\n",
      "3 ) 즐거운 하루가 되기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"안녕하세요. 좋은 아침입니다. 즐거운 하루가 되기 바랍니다.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef492ea",
   "metadata": {},
   "source": [
    "### 5) 등록되지 않은 약어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7250a433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) She holds an MDS.\n",
      "2 ) in Oral Pathology.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"She holds an MDS. in Oral Pathology.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92058b5",
   "metadata": {},
   "source": [
    "### 6) 좀 더 큰 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae36654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Backgammon is one of the oldest known board games.\n",
      "2 ) Its history can be tracked back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "3 ) It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Backgammon is one of the oldest known board games. Its history can be tracked back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bec5",
   "metadata": {},
   "source": [
    "## 2. Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67d6f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ee5baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- word list ----\n",
      "1 ) Backgammon\n",
      "2 ) is\n",
      "3 ) one\n",
      "4 ) of\n",
      "5 ) the\n",
      "6 ) oldest\n",
      "7 ) known\n",
      "8 ) board\n",
      "9 ) games\n",
      "10 ) .\n",
      "11 ) Its\n",
      "12 ) history\n",
      "13 ) can\n",
      "14 ) be\n",
      "15 ) tracked\n",
      "16 ) back\n",
      "17 ) nearly\n",
      "18 ) 5,000\n",
      "19 ) years\n",
      "20 ) to\n",
      "21 ) archeological\n",
      "22 ) discoveries\n",
      "23 ) in\n",
      "24 ) the\n",
      "25 ) Middle\n",
      "26 ) East\n",
      "27 ) .\n",
      "28 ) It\n",
      "29 ) is\n",
      "30 ) a\n",
      "31 ) two\n",
      "32 ) player\n",
      "33 ) game\n",
      "34 ) where\n",
      "35 ) each\n",
      "36 ) player\n",
      "37 ) has\n",
      "38 ) fifteen\n",
      "39 ) checkers\n",
      "40 ) which\n",
      "41 ) move\n",
      "42 ) between\n",
      "43 ) twenty-four\n",
      "44 ) points\n",
      "45 ) according\n",
      "46 ) to\n",
      "47 ) the\n",
      "48 ) roll\n",
      "49 ) of\n",
      "50 ) two\n",
      "51 ) dice\n",
      "52 ) .\n"
     ]
    }
   ],
   "source": [
    "print('---- word list ----')\n",
    "word_list = word_tokenize(EXAMPLE_TEXT)\n",
    "show_list(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dcd0499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- word list ----\n",
      "1 ) Today\n",
      "2 ) is\n",
      "3 ) a\n",
      "4 ) great\n",
      "5 ) day\n",
      "6 ) .\n",
      "1 ) It\n",
      "2 ) is\n",
      "3 ) even\n",
      "4 ) better\n",
      "5 ) than\n",
      "6 ) yesterday\n",
      "7 ) .\n",
      "1 ) And\n",
      "2 ) yesterday\n",
      "3 ) was\n",
      "4 ) the\n",
      "5 ) best\n",
      "6 ) day\n",
      "7 ) ever\n",
      "8 ) .\n"
     ]
    }
   ],
   "source": [
    "print('---- word list ----')\n",
    "for sent in sent_list:\n",
    "    word_list = word_tokenize(sent)\n",
    "    show_list(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd51f4",
   "metadata": {},
   "source": [
    "### 약어가 있는 문장에서 word 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48737e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Last\n",
      "2 ) night\n",
      "3 ) ,\n",
      "4 ) I\n",
      "5 ) went\n",
      "6 ) to\n",
      "7 ) Mrs.\n",
      "8 ) Martinez\n",
      "9 ) 's\n",
      "10 ) housewarming\n",
      "11 ) .\n",
      "12 ) It\n",
      "13 ) was\n",
      "14 ) a\n",
      "15 ) disaster\n",
      "16 ) .\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Last night, I went to Mrs. Martinez's housewarming. It was a disaster.\"\n",
    "show_list(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa104406",
   "metadata": {},
   "source": [
    "### 한글에서 word 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5066dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 안녕하세요\n",
      "2 ) .\n",
      "3 ) 좋은\n",
      "4 ) 아침입니다\n",
      "5 ) .\n",
      "6 ) 즐거운\n",
      "7 ) 하루가\n",
      "8 ) 되기\n",
      "9 ) 바랍니다\n",
      "10 ) .\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"안녕하세요. 좋은 아침입니다. 즐거운 하루가 되기 바랍니다.\"\n",
    "show_list(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e13fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benny",
   "language": "python",
   "name": "benny"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
