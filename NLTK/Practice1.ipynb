{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bbe407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e82a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "485ebdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_list(aList):\n",
    "    i = 1\n",
    "    for element in aList:\n",
    "        print(i, ')', element)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528743a2",
   "metadata": {},
   "source": [
    "## 1. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "171eb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d678d71",
   "metadata": {},
   "source": [
    "### 1) '.'로 문장 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c968d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = 'Today is a great day. It is even better than yesterday. And yesterday was the best day ever.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5b9cd895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- sentence list ----\n",
      "1 ) Today is a great day.\n",
      "2 ) It is even better than yesterday.\n",
      "3 ) And yesterday was the best day ever.\n"
     ]
    }
   ],
   "source": [
    "print('---- sentence list ----')\n",
    "sent_list = sent_tokenize(EXAMPLE_TEXT)\n",
    "show_list(sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3bbbb",
   "metadata": {},
   "source": [
    "### 2) '?', '!' 등으로 문장 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad6f1fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Hi, how are you?\n",
      "2 ) I'm good, you?\n",
      "3 ) Great!\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Hi, how are you? I'm good, you? Great!\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf31d7",
   "metadata": {},
   "source": [
    "### 3) 약어(Mr., Mrs., ..) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29dcbfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Last night, I went to Mrs. Martinez's housewarming.\n",
      "2 ) It was a disaster.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Last night, I went to Mrs. Martinez's housewarming. It was a disaster.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95171f3b",
   "metadata": {},
   "source": [
    "### 4) 한글 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8110d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 안녕하세요.\n",
      "2 ) 좋은 아침입니다.\n",
      "3 ) 즐거운 하루가 되기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"안녕하세요. 좋은 아침입니다. 즐거운 하루가 되기 바랍니다.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef492ea",
   "metadata": {},
   "source": [
    "### 5) 등록되지 않은 약어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7250a433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) She holds an MDS.\n",
      "2 ) in Oral Pathology.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"She holds an MDS. in Oral Pathology.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92058b5",
   "metadata": {},
   "source": [
    "### 6) 좀 더 큰 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aae36654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Backgammon is one of the oldest known board games.\n",
      "2 ) Its history can be tracked back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "3 ) It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Backgammon is one of the oldest known board games. Its history can be tracked back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\n",
    "show_list(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bec5",
   "metadata": {},
   "source": [
    "## 2. Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67d6f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7ee5baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- word list ----\n",
      "1 ) Backgammon\n",
      "2 ) is\n",
      "3 ) one\n",
      "4 ) of\n",
      "5 ) the\n",
      "6 ) oldest\n",
      "7 ) known\n",
      "8 ) board\n",
      "9 ) games\n",
      "10 ) .\n",
      "11 ) Its\n",
      "12 ) history\n",
      "13 ) can\n",
      "14 ) be\n",
      "15 ) tracked\n",
      "16 ) back\n",
      "17 ) nearly\n",
      "18 ) 5,000\n",
      "19 ) years\n",
      "20 ) to\n",
      "21 ) archeological\n",
      "22 ) discoveries\n",
      "23 ) in\n",
      "24 ) the\n",
      "25 ) Middle\n",
      "26 ) East\n",
      "27 ) .\n",
      "28 ) It\n",
      "29 ) is\n",
      "30 ) a\n",
      "31 ) two\n",
      "32 ) player\n",
      "33 ) game\n",
      "34 ) where\n",
      "35 ) each\n",
      "36 ) player\n",
      "37 ) has\n",
      "38 ) fifteen\n",
      "39 ) checkers\n",
      "40 ) which\n",
      "41 ) move\n",
      "42 ) between\n",
      "43 ) twenty-four\n",
      "44 ) points\n",
      "45 ) according\n",
      "46 ) to\n",
      "47 ) the\n",
      "48 ) roll\n",
      "49 ) of\n",
      "50 ) two\n",
      "51 ) dice\n",
      "52 ) .\n"
     ]
    }
   ],
   "source": [
    "print('---- word list ----')\n",
    "word_list = word_tokenize(EXAMPLE_TEXT)\n",
    "show_list(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2dcd0499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- word list ----\n",
      "1 ) Today\n",
      "2 ) is\n",
      "3 ) a\n",
      "4 ) great\n",
      "5 ) day\n",
      "6 ) .\n",
      "1 ) It\n",
      "2 ) is\n",
      "3 ) even\n",
      "4 ) better\n",
      "5 ) than\n",
      "6 ) yesterday\n",
      "7 ) .\n",
      "1 ) And\n",
      "2 ) yesterday\n",
      "3 ) was\n",
      "4 ) the\n",
      "5 ) best\n",
      "6 ) day\n",
      "7 ) ever\n",
      "8 ) .\n"
     ]
    }
   ],
   "source": [
    "print('---- word list ----')\n",
    "for sent in sent_list:\n",
    "    word_list = word_tokenize(sent)\n",
    "    show_list(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd51f4",
   "metadata": {},
   "source": [
    "### 약어가 있는 문장에서 word 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "48737e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Last\n",
      "2 ) night\n",
      "3 ) ,\n",
      "4 ) I\n",
      "5 ) went\n",
      "6 ) to\n",
      "7 ) Mrs.\n",
      "8 ) Martinez\n",
      "9 ) 's\n",
      "10 ) housewarming\n",
      "11 ) .\n",
      "12 ) It\n",
      "13 ) was\n",
      "14 ) a\n",
      "15 ) disaster\n",
      "16 ) .\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"Last night, I went to Mrs. Martinez's housewarming. It was a disaster.\"\n",
    "show_list(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa104406",
   "metadata": {},
   "source": [
    "### 한글에서 word 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c5066dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) 안녕하세요\n",
      "2 ) .\n",
      "3 ) 좋은\n",
      "4 ) 아침입니다\n",
      "5 ) .\n",
      "6 ) 즐거운\n",
      "7 ) 하루가\n",
      "8 ) 되기\n",
      "9 ) 바랍니다\n",
      "10 ) .\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT = \"안녕하세요. 좋은 아침입니다. 즐거운 하루가 되기 바랍니다.\"\n",
    "show_list(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c34495",
   "metadata": {},
   "source": [
    "## 3. Text Lemmantizing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0e25a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d606c",
   "metadata": {},
   "source": [
    "### 1) Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26ca40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "81062bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loving -> love\n",
      "trainee -> traine\n",
      "syllabi -> syllabi\n",
      "formulae -> formula\n",
      "criteria -> criteria\n",
      "believes -> believ\n",
      "writes -> write\n",
      "writing -> write\n",
      "write -> write\n"
     ]
    }
   ],
   "source": [
    "print('loving ->', stemmer.stem('loving'))\n",
    "print('trainee ->', stemmer.stem('trainee'))\n",
    "print('syllabi ->', stemmer.stem('syllabi'))\n",
    "print('formulae ->', stemmer.stem('formulae'))\n",
    "print('criteria ->', stemmer.stem('criteria'))\n",
    "print('believes ->', stemmer.stem('believes'))\n",
    "print('writes ->', stemmer.stem('writes'))\n",
    "print('writing ->', stemmer.stem('writing'))\n",
    "print('write ->', stemmer.stem('write'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf756c",
   "metadata": {},
   "source": [
    "### 2) Lemmantizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7985e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2022e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believes -> belief\n",
      "this -> this\n"
     ]
    }
   ],
   "source": [
    "print('believes ->', lemmatizer.lemmatize('believes'))\n",
    "print('this ->', lemmatizer.lemmatize('this'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f11c29ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believes 명사 -> belief\n",
      "believes 동사 -> believe\n",
      "crossing 형용사 -> crossing\n",
      "crossing 동사 -> cross\n",
      "crossing 명사 -> crossing\n",
      "crossing 부사 -> crossing\n"
     ]
    }
   ],
   "source": [
    "print('believes 명사 ->', lemmatizer.lemmatize('believes', pos='n'))  # noun\n",
    "print('believes 동사 ->', lemmatizer.lemmatize('believes', pos='v'))  # verb\n",
    "print('crossing 형용사 ->', lemmatizer.lemmatize('crossing', pos='a'))  # adjective\n",
    "print('crossing 동사 ->', lemmatizer.lemmatize('crossing', pos='v'))  # verb\n",
    "print('crossing 명사 ->', lemmatizer.lemmatize('crossing', pos='n'))  # noun\n",
    "print('crossing 부사 ->', lemmatizer.lemmatize('crossing', pos='r'))  # adverb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f85c9a",
   "metadata": {},
   "source": [
    "### 3) Stemming vs. Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a6ff4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_stemmer_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    print(\"Stemmer: \", stemmer.stem(word))\n",
    "    print(\"Lemmatizer: \", lemmatizer.lemmatize(word, pos))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d26e0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer:  seen\n",
      "Lemmatizer:  see\n",
      "\n",
      "Stemmer:  drive\n",
      "Lemmatizer:  drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_stemmer_lemmatizer(stemmer, lemmatizer, \"seen\", \"v\")\n",
    "compare_stemmer_lemmatizer(stemmer, lemmatizer, \"drive\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966525e",
   "metadata": {},
   "source": [
    "### 4) Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08054937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- stemmer -------\n",
      "deactiv\n",
      "deactiv\n",
      "deactiv\n"
     ]
    }
   ],
   "source": [
    "print('------- stemmer -------')\n",
    "print(stemmer.stem(\"deactivating\"))\n",
    "print(stemmer.stem(\"deactivated\"))\n",
    "print(stemmer.stem(\"deactivates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89176049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- lemmatizer -------\n",
      "deactivate\n",
      "deactivate\n",
      "deactivate\n"
     ]
    }
   ],
   "source": [
    "print('------- lemmatizer -------')\n",
    "print(lemmatizer.lemmatize(\"deactivating\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"deactivated\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"deactivates\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4a3b790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- stemmer -------\n",
      "stones -> stone\n",
      "speaking -> speak\n",
      "bedroom -> bedroom\n",
      "jokes -> joke\n",
      "lisa -> lisa\n",
      "purple -> purpl\n"
     ]
    }
   ],
   "source": [
    "print('------- stemmer -------')\n",
    "print('stones ->', stemmer.stem('stones'))\n",
    "print('speaking ->', stemmer.stem('speaking'))\n",
    "print('bedroom ->', stemmer.stem('bedroom'))\n",
    "print('jokes ->', stemmer.stem('jokes'))\n",
    "print('lisa ->', stemmer.stem('lisa'))\n",
    "print('purple ->', stemmer.stem('purple'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e0da4506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- lemmatizer -------\n",
      "stones -> stone\n",
      "speaking 명사 -> speaking\n",
      "speaking 동사 -> speak\n",
      "bedroom -> bedroom\n",
      "jokes -> joke\n",
      "lisa -> lisa\n",
      "purple -> purple\n"
     ]
    }
   ],
   "source": [
    "print('------- lemmatizer -------')\n",
    "print('stones ->', lemmatizer.lemmatize('stones', 'n'))\n",
    "print('speaking 명사 ->', lemmatizer.lemmatize('speaking', 'n'))\n",
    "print('speaking 동사 ->', lemmatizer.lemmatize('speaking', 'v'))\n",
    "print('bedroom ->', lemmatizer.lemmatize('bedroom', 'n'))\n",
    "print('jokes ->', lemmatizer.lemmatize('jokes', 'n'))\n",
    "print('lisa ->', lemmatizer.lemmatize('lisa', 'n'))\n",
    "print('purple ->', lemmatizer.lemmatize('purple', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e6f61",
   "metadata": {},
   "source": [
    "## 4. Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd059272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bf626cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "890a7ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab4a292d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac096",
   "metadata": {},
   "source": [
    "### Text에서 stop words 제거 -> 나머지 단어들만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c7f2c139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- filtered words -----\n",
      "1 ) Today\n",
      "2 ) great\n",
      "3 ) day\n",
      "4 ) .\n",
      "5 ) It\n",
      "6 ) even\n",
      "7 ) better\n",
      "8 ) yesterday\n",
      "9 ) .\n",
      "10 ) And\n",
      "11 ) yesterday\n",
      "12 ) best\n",
      "13 ) day\n",
      "14 ) ever\n",
      "15 ) !\n"
     ]
    }
   ],
   "source": [
    "text = 'Today is a great day. It is even better than yesterday. And yesterday was the best day ever!'\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filteredWords = []\n",
    "for w in words:\n",
    "    if w not in stopwords_list:\n",
    "        filteredWords.append(w)\n",
    "\n",
    "print('----- filtered words -----')\n",
    "show_list(filteredWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8346b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list.append('even')\n",
    "stopwords_list.append('and')\n",
    "stopwords_list.append('ever')\n",
    "stopwords_list.append('it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14bad6d",
   "metadata": {},
   "source": [
    "### 더 할일\n",
    "- 단어가 아닌 것 제거: ., !, ?, 12, ... isalpha()\n",
    "- 소문자로 변경 lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5f19487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'korea'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Korea'\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "52d84bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Korea'\n",
    "s.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89baf31",
   "metadata": {},
   "source": [
    "## 5. NLTK WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "01fb79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acef69",
   "metadata": {},
   "source": [
    "### 1) synset (유사어 리스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6829bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) Synset('life.n.01')\n",
      "2 ) Synset('life.n.02')\n",
      "3 ) Synset('life.n.03')\n",
      "4 ) Synset('animation.n.01')\n",
      "5 ) Synset('life.n.05')\n",
      "6 ) Synset('life.n.06')\n",
      "7 ) Synset('life.n.07')\n",
      "8 ) Synset('life.n.08')\n",
      "9 ) Synset('liveliness.n.02')\n",
      "10 ) Synset('life.n.10')\n",
      "11 ) Synset('life.n.11')\n",
      "12 ) Synset('biography.n.01')\n",
      "13 ) Synset('life.n.13')\n",
      "14 ) Synset('life_sentence.n.01')\n"
     ]
    }
   ],
   "source": [
    "synset = wordnet.synsets('life')\n",
    "show_list(synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc243bf6",
   "metadata": {},
   "source": [
    "### 2) synset 사례, 단어, 정의, 용례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "43089d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st synset example: life.n.01\n",
      "word itself: life\n",
      "definition: a characteristic state or mode of living\n",
      "examples: ['social life', 'city life', 'real life']\n"
     ]
    }
   ],
   "source": [
    "print('1st synset example:', synset[0].name())\n",
    "print('word itself:', synset[0].lemmas()[0].name())\n",
    "print('definition:', synset[0].definition())\n",
    "print('examples:', synset[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16fa8c",
   "metadata": {},
   "source": [
    "### 3) synonym list 얻기: lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a594db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) large\n",
      "2 ) large\n",
      "3 ) big\n",
      "4 ) large\n",
      "5 ) bombastic\n",
      "6 ) declamatory\n",
      "7 ) large\n",
      "8 ) orotund\n",
      "9 ) tumid\n",
      "10 ) turgid\n",
      "11 ) big\n",
      "12 ) large\n",
      "13 ) magnanimous\n",
      "14 ) big\n",
      "15 ) large\n",
      "16 ) prominent\n",
      "17 ) large\n",
      "18 ) big\n",
      "19 ) enceinte\n",
      "20 ) expectant\n",
      "21 ) gravid\n",
      "22 ) great\n",
      "23 ) large\n",
      "24 ) heavy\n",
      "25 ) with_child\n",
      "26 ) large\n",
      "27 ) large\n",
      "28 ) boastfully\n",
      "29 ) vauntingly\n",
      "30 ) big\n",
      "31 ) large\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('large'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "show_list(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d543d",
   "metadata": {},
   "source": [
    "### 4) antonym list: lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3e6740c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small', 'little']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonyms = []\n",
    "for syn in wordnet.synsets('large'):\n",
    "    for lemma in syn.lemmas():\n",
    "        if lemma.antonyms():\n",
    "            antonyms.append(lemma.antonyms()[0].name())\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44b1e3",
   "metadata": {},
   "source": [
    "### 5) 단어 유사도 비교: ship, boat, car, cat => Wu and Palmer method for semantic relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e47736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('boat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "369d717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('car.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "76d70654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('ship.n.01')\n",
    "w2 = wordnet.synset('cat.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416ffdf",
   "metadata": {},
   "source": [
    "#### coffee, cafe, football, soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "23121ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('coffee.n.01')\n",
    "w2 = wordnet.synset('cafe.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bcf43ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21052631578947367\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('coffee.n.01')\n",
    "w2 = wordnet.synset('football.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e8c3bff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('coffee.n.01')\n",
    "w2 = wordnet.synset('soccer.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bcc2cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('football.n.01')\n",
    "w2 = wordnet.synset('soccer.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cfd32819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10526315789473684\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('football.n.01')\n",
    "w2 = wordnet.synset('cafe.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "96939c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset('soccer.n.01')\n",
    "w2 = wordnet.synset('cafe.n.01')\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dec752",
   "metadata": {},
   "source": [
    "참고: https://www.youtube.com/watch?v=p1ETojsnXYk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8453c4",
   "metadata": {},
   "source": [
    "## 6. Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a9abb308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4ea64409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('even', 'RB'),\n",
       " ('better', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('yesterday', 'NN'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('yesterday', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('best', 'JJS'),\n",
       " ('day', 'NN'),\n",
       " ('ever', 'RB'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Today is a great day. It is even better than yesterday. And yesterday was the best day ever!'\n",
    "word_list = word_tokenize(text)\n",
    "pos_tag_list = pos_tag(word_list)\n",
    "pos_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4eb63459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Today', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), ('is', 'VBZ'), ('even', 'RB'), ('better', 'JJR'), ('than', 'IN'), ('yesterday', 'NN'), ('.', '.')]\n",
      "[('And', 'CC'), ('yesterday', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('best', 'JJS'), ('day', 'NN'), ('ever', 'RB'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_list = []\n",
    "sent_list = sent_tokenize(text)\n",
    "for sent in sent_list:\n",
    "    word_list = word_tokenize(sent)\n",
    "    pos_tag_list = pos_tag(word_list)\n",
    "    print(pos_tag_list)\n",
    "    tagged_list.append(pos_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d6268",
   "metadata": {},
   "source": [
    "## 7. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "299b1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5e35e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = ('''NP: {<DT>?<JJ>*<NN>} # NP''')\n",
    "chunkParser = RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "03df4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Today/NN) is/VBZ (NP a/DT great/JJ day/NN) ./.)\n",
      "(NP Today/NN)\n",
      "(NP a/DT great/JJ day/NN)\n",
      "\n",
      "(S It/PRP is/VBZ even/RB better/JJR than/IN (NP yesterday/NN) ./.)\n",
      "(NP yesterday/NN)\n",
      "\n",
      "(S\n",
      "  And/CC\n",
      "  (NP yesterday/NN)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  best/JJS\n",
      "  (NP day/NN)\n",
      "  ever/RB\n",
      "  !/.)\n",
      "(NP yesterday/NN)\n",
      "(NP day/NN)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tagged in tagged_list:\n",
    "    tree = chunkParser.parse(tagged)\n",
    "\n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e444ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f07078",
   "metadata": {},
   "source": [
    "## 8. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ded54dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2748af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  who/WP\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Yahoo/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  who/WP\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  decided/VBD\n",
      "  to/TO\n",
      "  meet/VB\n",
      "  at/IN\n",
      "  (GPE New/NNP York/NNP City/NNP))\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mark who works at Yahoo and John who works at Google decided to meet at New York City\"\n",
    "words = word_tokenize(sentence)\n",
    "tags = pos_tag(words)\n",
    "ne = ne_chunk(tags)\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85073f0b",
   "metadata": {},
   "source": [
    "## 9. Bag-of-Words\n",
    "- text를 vectors of numbers로 변환하는 하나의 방법\n",
    "- vocabulary를 정의해야 함(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514a9a7",
   "metadata": {},
   "source": [
    "### 1) read file and split by lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ed791e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) I like this movie, It is funny movie.\n",
      "2 ) I hate this movie.\n",
      "3 ) This was awesome! I like it.\n",
      "4 ) Nice one. I love it.\n"
     ]
    }
   ],
   "source": [
    "with open('sample.txt', 'r') as file:\n",
    "    lines = file.read().splitlines()\n",
    "show_list(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17789e07",
   "metadata": {},
   "source": [
    "### 2) vocabulary 정의 -> document vector 생성\n",
    "- sample.txt에 있는 모든 단어들: 마침표, 기호 등 제외\n",
    "- sklearn library의 CountVectorizer class 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "83a2c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8a67e83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bag_of_words = cv.fit_transform(lines)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1379767f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   awesome  funny  hate  is  it  like  love  movie  nice  one  this  was\n",
       "0        0      1     0   1   1     1     0      2     0    0     1    0\n",
       "1        0      0     1   0   0     0     0      1     0    0     1    0\n",
       "2        1      0     0   0   1     1     0      0     0    0     1    1\n",
       "3        0      0     0   0   1     0     1      0     1    1     0    0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe3134",
   "metadata": {},
   "source": [
    "## 10. TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "- sklearn의 TfidfVectorizer class 이용\n",
    "- 단어의 중요성 여부를 알려주는 척도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e1529f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "926b2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiv = TfidfVectorizer()\n",
    "values = tiv.fit_transform(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "be300012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410901</td>\n",
       "      <td>0.262273</td>\n",
       "      <td>0.323959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262273</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        is        it      like      love  \\\n",
       "0  0.000000  0.410901  0.000000  0.410901  0.262273  0.323959  0.000000   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.539445  0.000000  0.000000  0.000000  0.344321  0.425305  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736   \n",
       "\n",
       "      movie      nice       one      this       was  \n",
       "0  0.647918  0.000000  0.000000  0.262273  0.000000  \n",
       "1  0.553492  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.000000  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tiv.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benny",
   "language": "python",
   "name": "benny"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
